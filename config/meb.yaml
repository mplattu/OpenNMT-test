## Where the samples will be written
save_data: run/meb
## Where the vocab(s) will be written
src_vocab: run/meb.vocab.src
tgt_vocab: run/meb.vocab.tgt
# Prevent overwriting existing files in the folder
overwrite: False

# Corpus opts:
data:
    corpus_1:
        path_src: finlex/src-train.txt
        path_tgt: finlex/tgt-train.txt
        weight: 1
    corpus_2:
        path_src: ccmatrix/src-train.txt
        path_tgt: ccmatrix/tgt-train.txt
        weight: 1
    valid:
        path_src: src-val.txt
        path_tgt: tgt-val.txt

# Vocabulary files that were just created
src_vocab: run/meb.vocab.src
tgt_vocab: run/meb.vocab.tgt

# Train on a single GPU
world_size: 1
gpu_ranks: [0]

# Where to save the checkpoints
save_model: run/model
save_checkpoint_steps: 1000
train_steps: 100000
valid_steps: 5000

# To avoid CUDA running out of memory

# Size of queue for each process in producer/consumer
# Default: 40
#queue_size: 40

# Examples per dynamically generated torchtext Dataset.
# Default: 2048
#bucket_size: 2048

# Possible choices: sents, tokens
# Batch grouping for batch_size. Standard is sents. Tokens will do dynamic batching
# Default: “sents”
#batch_type: "sents"

# Maximum batch size for training
# Default: 64
#batch_size: 64

# Maximum batch size for validation
# Default: 32
#valid_batch_size: 32

# Batch size multiple for token batches.
#batch_size_multiple: 1

# Maximum batches of words in a sequence to run the generator on in parallel. Higher is faster, but uses more memory. Set to 0 to disable.
# Default: 32
#max_generator_batches: 32

# Accumulate gradient this many times. Approximately equivalent to updating batch_size * accum_count batches at once. Recommended for Transformer.
# Default: [1]
#accum_count: [1]

# Steps at which accum_count values change
# Default: [0]
#accum_steps: [0]
